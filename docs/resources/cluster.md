---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "anaml-operations_cluster Resource - terraform-provider-anaml-operations"
subcategory: ""
description: |-
  Clusters
  A Cluster represents an external compute resource that can be used to compute feature values,
  run monitoring jobs and and generate previews.
  There are two types of Clusters, a local cluster, and an Anaml Spark Server.
  Local Clusters
  Local Clusters are rarely used outside of a development environment, and use the Anaml Server
  application as a Spark Cluster in local mode.
  Anaml Spark Server
  This form of cluster uses a separate microservice which runs a web application and launches
  jobs on a Spark cluster. For this form of Cluster, you're required to provide the URL of the
  spark server application.
  Anaml Spark Server Clusters can be:
  Google Dataproc clustersAmazon EMR clustersAzure HD Insight clustersHadoop Yarn clustersSpark on Kubernetes clusters
---

# anaml-operations_cluster (Resource)

# Clusters

A Cluster represents an external compute resource that can be used to compute feature values,
run monitoring jobs and and generate previews.

There are two types of Clusters, a local cluster, and an Anaml Spark Server.

### Local Clusters

Local Clusters are rarely used outside of a development environment, and use the Anaml Server
application as a Spark Cluster in local mode.

### Anaml Spark Server

This form of cluster uses a separate microservice which runs a web application and launches
jobs on a Spark cluster. For this form of Cluster, you're required to provide the URL of the
spark server application.

Anaml Spark Server Clusters can be:

- Google Dataproc clusters
- Amazon EMR clusters
- Azure HD Insight clusters
- Hadoop Yarn clusters
- Spark on Kubernetes clusters



<!-- schema generated by tfplugindocs -->
## Schema

### Required

- **description** (String)
- **is_preview_cluster** (Boolean) Whether this cluster can be used for preview generation.
- **name** (String) The name of the cluster.
- **spark_config** (Block List, Min: 1, Max: 1) Additional configuration which is passed to Spark when performing feature generation runs. (see [below for nested schema](#nestedblock--spark_config))

### Optional

- **attribute** (Block List) Attributes (key value pairs) to attach to the object (see [below for nested schema](#nestedblock--attribute))
- **id** (String) The ID of this resource.
- **labels** (List of String) Labels to attach to the object
- **local** (Block List, Max: 1) Set up for a local cluster. When this setting is used, a local spark session will be launched within the JVM process of the web server. Not recommended for production deployments. (see [below for nested schema](#nestedblock--local))
- **spark_server** (Block List, Max: 1) Set up for a remote cluster. (see [below for nested schema](#nestedblock--spark_server))

<a id="nestedblock--spark_config"></a>
### Nested Schema for `spark_config`

Required:

- **enable_hive_support** (Boolean)

Optional:

- **additional_spark_properties** (Map of String)
- **hive_metastore_url** (String)


<a id="nestedblock--attribute"></a>
### Nested Schema for `attribute`

Required:

- **key** (String)

Optional:

- **value** (String)


<a id="nestedblock--local"></a>
### Nested Schema for `local`

Required:

- **anaml_server_url** (String)

Optional:

- **aws** (Block List, Max: 1) (see [below for nested schema](#nestedblock--local--aws))
- **basic** (Block List, Max: 1) (see [below for nested schema](#nestedblock--local--basic))
- **gcp** (Block List, Max: 1) (see [below for nested schema](#nestedblock--local--gcp))

<a id="nestedblock--local--aws"></a>
### Nested Schema for `local.aws`

Required:

- **password_secret_id** (String)
- **username** (String)


<a id="nestedblock--local--basic"></a>
### Nested Schema for `local.basic`

Required:

- **password** (String)
- **username** (String)


<a id="nestedblock--local--gcp"></a>
### Nested Schema for `local.gcp`

Required:

- **password_secret_id** (String)
- **password_secret_project** (String)
- **username** (String)



<a id="nestedblock--spark_server"></a>
### Nested Schema for `spark_server`

Required:

- **spark_server_url** (String)


